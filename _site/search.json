[
  {
    "objectID": "posts/consensusClustering/consensus_clustering.html",
    "href": "posts/consensusClustering/consensus_clustering.html",
    "title": "Circumventing poor mixing in Bayesian model-based clustering",
    "section": "",
    "text": "Paper: Consensus Clustering for Bayesian Mixture Models\n\nAim: Perform inference on Bayesian clustering methods when chains do not converge without reimplementation\nPros: Easy to use and offers useful inference\nCons: Inference is no longer Bayesian"
  },
  {
    "objectID": "posts/consensusClustering/consensus_clustering.html#introduction",
    "href": "posts/consensusClustering/consensus_clustering.html#introduction",
    "title": "Circumventing poor mixing in Bayesian model-based clustering",
    "section": "Introduction",
    "text": "Introduction\nThe first paper of my PhD was entitled “Consensus Clustering for Bayesian Mixture Models” and written with my supervisors Paul and Chris. To overcome the issue of poor mixing that haunts Bayesian clustering, we proposed a heuristic method based on the consensus clustering algorithm of Monti et al. (2003) that appears to enable principled inference without reimplementing the sampler. However, the inference does lose the Bayesian interpretation.\nIf you have attempted to infer discrete latent structure using a Bayesian method implemented using Markov Chain Monte Carlo (MCMC) methods, then you’ve probably encountered the problem of poor mixing where chains become trapped in different clusterings and stop exploring the parameter space.There is a large body of literature about methods for navigating this problem in a principled fashion (see, e.g., Dahl 2003; Jain and Neal 2004; Bouchard-Côté, Doucet, and Roth 2017; Syed et al. 2022), but these state-of-the-art samplers tend to be time-consuming to implement. In this paper we proposed taking samples from many (relatively) short chains rather than a single long chain. This idea has seen a lot of mileage recently across a few different papers, but where we differ is we surrender our claim of being truly Bayesian. We have to do this as, while it is possibly that our samples are drawn from the target density, they are unlikely to be weighted correctly. However, we found that the expected values from the distributions sampled this way tended to be correct even if the variance might be too large."
  },
  {
    "objectID": "posts/consensusClustering/consensus_clustering.html#algorithm",
    "href": "posts/consensusClustering/consensus_clustering.html#algorithm",
    "title": "Circumventing poor mixing in Bayesian model-based clustering",
    "section": "Algorithm",
    "text": "Algorithm\nThe basic method is sketched below:\n\nConsensus clustering for Bayesian methods\nInputs:\n  W: the number of chains to run\n  D: the number of iterations to run each chain for\n  sampler: the MCMC sampler for the method\n  priorDraw(): a method of drawing a sample from the prior distribution over the clustering space (this is often integrated into the sampler in practice)\n \nOutput:\n  c: a matrix of sampled partitions\n\nAlgorithm:\n  for(w in 1:W) {\n    # Differentiate the model runs using the random seed\n    set.seed(w);\n    c_0 <- prior();\n    for(r in 1:R) {\n      # Sample a new clustering based on the current partition\n      c_r <- sampler(c_r);\n    }\n    # Save the last iteration\n    c[w] <- c_r\n  }\n\nObviously one can sample more quantities than just the clustering (as we do in the paper), but I think that clustering works particularly well with short chains. This is because we often care about co-clustering probabilities across sampled partitions rather than any single sample being correct; this means that if there is enough variety across a large number of poor (but better than random) sampled partitions we might still infer the correct structure. For other variables we expect that we need longer chains to have the correct expected value from our sampled distribution. The Multiple Dataset Integration (MDI) example in the paper is a good example of this. MDI is an integrative or multi-view Bayesian clustering method. Signal is shared across datasets to inform clustering in each view and the degree to which information should be shared is also inferred. This is represented by the \\(\\phi\\) parameters in the model. To learn meaningful estimate of \\(\\phi\\) and infer dependent clusterings we need to go much further into the chains then we do for clustering a single dataset. This is because for the information to be shared and how it is shared depends on the global clustering structure emerging. This means that the Markov chains have to reach their target density for consensus clustering to offer a useful inference and the chains are long. This means we lose the massive speed gain that very short chains offer, but we still overcome the multi-modality so often present in the posterior distribution of complex latent structure models. Based on this result we suspect that the method generalises to any Bayesian model, but for this vignette I will only explore clustering of a single dataset."
  },
  {
    "objectID": "posts/consensusClustering/consensus_clustering.html#guessing-chain-length",
    "href": "posts/consensusClustering/consensus_clustering.html#guessing-chain-length",
    "title": "Circumventing poor mixing in Bayesian model-based clustering",
    "section": "Guessing chain length",
    "text": "Guessing chain length\nTo use consensus clustering, we want to have an idea of how long we will need to run the chains for. In this case we run a small number of chains for longer than necessary and the look at trace plots to see when they stopped exploring or the degree of exploration had tapered off significantly.\n\nn_initial_chains <- 9\nR_long <- 5000\nthin_long <- 50\niterations_initial <- seq(0, R_long, thin_long)\n\nn_samples <- floor(R_long / thin_long) + 1\ntype <- \"G\"\n\npsms <- initial_mcmc <- vector(\"list\", n_initial_chains)\nlikelihood_df <- matrix(0, nrow = n_samples - 1, ncol = n_initial_chains)\ncolnames(likelihood_df) <- paste0(\"Chain\", seq(1, n_initial_chains))\n\nari_mat <- concentration_df <- matrix(0, nrow = n_samples, ncol = n_initial_chains)\ncolnames(ari_mat) <- colnames(concentration_df) <- paste0(\"Chain\", seq(1, n_initial_chains))\n\nensemble_psm <- matrix(0, nrow = N, ncol = N)\n\nfor (ii in seq(1, n_initial_chains)) {\n  initial_mcmc[[ii]] <- .mcmc <- callMixtureModel(my_data, R_long, thin_long, type)\n  likelihood_df[, ii] <- .mcmc$complete_likelihood[-1]\n  concentration_df[, ii] <- .mcmc$mass\n  ari_mat[, ii] <- apply(.mcmc$allocations, 1, function(x) {mcclust::arandi(x, .mcmc$allocations[n_samples, ])})\n  .psm <- makePSM(initial_mcmc[[ii]]$allocations)\n  row.names(.psm) <- colnames(.psm) <- row.names(my_data)\n  psms[[ii]] <- .psm\n  ensemble_psm <- ensemble_psm + .psm\n}\n\nensemble_psm <- ensemble_psm / n_initial_chains\n\n\nari_df <- ari_mat |> \n  as.data.frame() |> \n  mutate(Iteration = iterations_initial) |>\n  pivot_longer(-Iteration, names_to = \"Chain\", values_to = \"ARI\")\n\nlikelihood_df2 <- likelihood_df |>\n  as.data.frame() |>\n  mutate(Iteration = iterations_initial[-1]) |>\n  pivot_longer(-Iteration, names_to = \"Chain\", values_to = \"Complete_log_likelihood\")\n\nmass_df <- concentration_df |>\n  as.data.frame() |>\n  mutate(Iteration = iterations_initial) |>\n  pivot_longer(-Iteration, names_to = \"Chain\", values_to = \"Mass\")\n\nlikelihood_df2 |>\n  ggplot(aes(x = Iteration, y = Complete_log_likelihood, group = Chain)) +\n  geom_line() +\n  ggthemes::scale_color_colorblind()\n\n\n\nmass_df |>\n  ggplot(aes(x = Iteration, y = Mass, group = Chain)) +\n  geom_line() +\n  ggthemes::scale_color_colorblind()\n\n\n\nari_df |> \n  ggplot(aes(x = Iteration, y = ARI, group = Chain)) +\n  geom_line() +\n  ggthemes::scale_color_colorblind() +\n  labs(title = \"ARI between rth sampled partition and final sampled partition in each chains\")\n\n\n\n\nThese long chains have stopped exploring new clusterings almost instantly. Two chains do one large change several thousand iterations into the sampler, but all chains find a partition similar to the final sampled partition within 1,000 iteration. The mass parameter takes awhile to converge, but this is sampled via Metropolis-Hastings, so this makes sense.\n\npsm <- makePSM(.mcmc$allocations)\nrow.names(psm) <- colnames(psm) <- row.names(my_data)\n\n# Create the heatmap of the PSM\nrow_order <- findOrder(psm)\npheatmap::pheatmap(psm[row_order, row_order],\n  color = simColPal(),\n  breaks = defineBreaks(simColPal(), lb = 0),\n  annotation_row = anno_row,\n  annotation_colors = ann_colours,\n  main = \"Example PSM annotated by clusterings\",\n  show_rownames = FALSE,\n  show_colnames = FALSE,\n  cluster_rows = FALSE,\n  cluster_cols = FALSE\n)\n\n\n\npsm_df <- prepSimilarityMatricesForGGplot(psms)\n\npsm_df |>\n  ggplot(aes(x = x, y = y, fill = Entry)) +\n  geom_tile() +\n  facet_wrap(~Chain) +\n  scale_fill_gradient(\n    low = \"#FFFFFF\",\n    high = \"#146EB4\"\n  ) +\n  labs(title = \"PSMs for initial chains with common ordering\") +\n  theme(axis.text.x=element_blank(),\n    axis.ticks.x=element_blank(),\n    axis.text.y=element_blank(),\n    axis.ticks.y=element_blank() \n  )\n\n\n\n\nThese PSMs also show why nested \\(\\hat{R}\\) and coupling don’t work for clustering. These methods assume that the different chains can eventually reach similar places; this is unlikely to happen in any reasonable length of time in even as low-dimensional an example as this (70 features; hardly low-dimensional in many settings, but that’s one of the joys of ’omics). These PSMs essentially binary, suggesting the same partition is sampled repeatedly; only chains 7 and 8 appear to capture any variety in the clusterings."
  },
  {
    "objectID": "posts/consensusClustering/consensus_clustering.html#depth-tests",
    "href": "posts/consensusClustering/consensus_clustering.html#depth-tests",
    "title": "Circumventing poor mixing in Bayesian model-based clustering",
    "section": "Depth tests",
    "text": "Depth tests\nEssentially we will follow the logic of Geweke and compare the distribution using early samples to later samples. We differ in that the two sets of samples are collected across many chains, but I think the logic holds. Here we use a Cranmer-von Mises test to test if the sampled distributions of the mass parameter for the clustering and the complete log-likelihood are the same in earlier samples to the same distributions for the final sample from all the chains.\n\nm1 <- mass_df$Mass[mass_df$D == 100]\nm2 <- mass_df$Mass[mass_df$D == 300]\nm3 <- mass_df$Mass[mass_df$D == 600]\nm4 <- mass_df$Mass[mass_df$D == 1000]\n\nl1 <- lkl_df$Complete_log_likelihood[lkl_df$D == 100]\nl2 <- lkl_df$Complete_log_likelihood[lkl_df$D == 300]\nl3 <- lkl_df$Complete_log_likelihood[lkl_df$D == 600]\nl4 <- lkl_df$Complete_log_likelihood[lkl_df$D == 1000]\n\nt_d1_m_1 <- cvm_test(m1, m4)\nt_d1_m_2 <- cvm_test(m2, m4)\nt_d1_m_3 <- cvm_test(m3, m4)\n\nt_d1_l_1 <- cvm_test(l1, l4)\nt_d1_l_2 <- cvm_test(l2, l4)\nt_d1_l_3 <- cvm_test(l3, l4)"
  },
  {
    "objectID": "posts/consensusClustering/consensus_clustering.html#width-tests",
    "href": "posts/consensusClustering/consensus_clustering.html#width-tests",
    "title": "Circumventing poor mixing in Bayesian model-based clustering",
    "section": "Width tests",
    "text": "Width tests\nTesting if we are using a sufficient quantity of chains is harder as the chains are not ordered. I believe we can essentially copy the Geweke test again but compare disjoint subsets of samples. For example, take the final sample from a random two fifths of the chains and compare the distribution for these to the distribution from the remaining three fifths and then repeat this multiple times.\n\nfrac_used <- 0.4\ninds1 <- sample(seq(1, W), size = W * frac_used, replace = FALSE)\ninds2 <- sample(seq(1, W), size = W * frac_used, replace = FALSE)\ninds3 <- sample(seq(1, W), size = W * frac_used, replace = FALSE)\ninds4 <- sample(seq(1, W), size = W * frac_used, replace = FALSE)\ninds5 <- sample(seq(1, W), size = W * frac_used, replace = FALSE)\n\nt_w1_m <- cvm_test(m3[inds1], m3[-inds1])\nt_w1_l <- cvm_test(l3[inds1], l3[-inds1])\n\nt_w2_m <- cvm_test(m3[inds2], m3[-inds2])\nt_w2_l <- cvm_test(l3[inds2], l3[-inds2])\n\nt_w3_m <- cvm_test(m3[inds3], m3[-inds3])\nt_w3_l <- cvm_test(l3[inds3], l3[-inds3])\n\nt_w4_m <- cvm_test(m3[inds4], m3[-inds4])\nt_w4_l <- cvm_test(l3[inds4], l3[-inds4])\n\nt_w5_m <- cvm_test(m3[inds5], m3[-inds5])\nt_w5_l <- cvm_test(l3[inds5], l3[-inds5])\n\n\ntest_df <- data.frame(\n  \"D\" = c(100, 300, 600, D, D, D, D, D),\n  \"W\" = c(W, W, W, 0.2 * W, 0.2 * W, 0.2 * W, 0.2 * W, 0.2 * W),\n  \"Mass\" = c(\n    t_d1_m_1[2],\n    t_d1_m_2[2],\n    t_d1_m_3[2],\n    t_w1_m[2],\n    t_w2_m[2],\n    t_w3_m[2],\n    t_w4_m[2],\n    t_w5_m[2]\n  ),\n  \"Complete_log_likelihood\" = c(\n    t_d1_l_1[2],\n    t_d1_l_2[2],\n    t_d1_l_3[2],\n    t_w1_l[2],\n    t_w2_l[2],\n    t_w3_l[2],\n    t_w4_l[2],\n    t_w5_l[2]\n  )\n)\n\n# test_df$Mass <- round(test_df$Mass, 4)\n# test_df$Complete_log_likelihood <- round(test_df$Complete_log_likelihood, 4)\n\nNow the p values for the mass parameter and complete log-likelihood t-tests are shown in the below table.\n\nknitr::kable(test_df, caption = \"Convergence tests for consensus clustering\", digits = 3)\n\n\nConvergence tests for consensus clustering\n\n\nD\nW\nMass\nComplete_log_likelihood\n\n\n\n\n100\n500\n0.000\n0.629\n\n\n300\n500\n0.000\n0.912\n\n\n600\n500\n0.136\n1.000\n\n\n1000\n100\n0.920\n0.570\n\n\n1000\n100\n0.554\n0.766\n\n\n1000\n100\n0.519\n0.098\n\n\n1000\n100\n0.194\n0.025\n\n\n1000\n100\n0.769\n0.202\n\n\n\n\n\nThe mass is sampled via Metropolis Hastings, so it taking longer to converge than the complete log-likelihood (which is a direct function of the clustering) is not shocking."
  },
  {
    "objectID": "posts/consensusClustering/consensus_clustering.html#mean-abolute-difference-between-consensus-matrices",
    "href": "posts/consensusClustering/consensus_clustering.html#mean-abolute-difference-between-consensus-matrices",
    "title": "Circumventing poor mixing in Bayesian model-based clustering",
    "section": "Mean abolute difference between consensus matrices",
    "text": "Mean abolute difference between consensus matrices\nIn the paper I recommended using the below kind of plots, but these need more models for assessing as with only three levels of both the iteration and number of chains used we only have two data points for comparisons, so they have some limitations.\n\nmean_abs_diff_df <- makeCMComparisonSummaryDF(cms, models)\n\nmean_abs_diff_df |>\n  dplyr::filter(Quantity_varied == \"Depth\") |>\n  ggplot(aes(x = Depth, y = Mean_absolute_difference, color = factor(Width))) +\n  geom_line() +\n  labs(title = \"Assessing stability across cms for increasing chain depth\") +\n  ggthemes::scale_color_colorblind()\n\n\n\nmean_abs_diff_df |>\n  dplyr::filter(Quantity_varied == \"Width\") |>\n  ggplot(aes(x = Width, y = Mean_absolute_difference, color = factor(Depth))) +\n  geom_line() +\n  labs(title = \"Assessing stability across cms for increasing numbers of chains\") +\n  ggthemes::scale_color_colorblind()"
  },
  {
    "objectID": "posts/mdir/mdir.html",
    "href": "posts/mdir/mdir.html",
    "title": "mdir: Bayesian multi-omics clustering",
    "section": "",
    "text": "This vignette introduces the mdir R package. This package offers software for Bayesian model based clustering and the Multiple Dataset Integration model for multi-omics analysis. It uses the C++17 parallel policies and thus is unavailable on MAC OS and Debian systems when used with the default C++ compilers."
  },
  {
    "objectID": "posts/mdir/mdir.html#mixture-models",
    "href": "posts/mdir/mdir.html#mixture-models",
    "title": "mdir: Bayesian multi-omics clustering",
    "section": "Mixture models",
    "text": "Mixture models\nLet \\(X=(X_1, \\ldots, X_N)\\) be the observed data, with \\(X_n = [X_{n, 1}, \\ldots, X_{n,P}]^\\top\\) for each item being considered, where each observation has \\(P\\) variables. We wish to model the data using a mixture of densities: \\[\\begin{align}\n    p(X_n | \\theta = \\{\\theta_1, \\ldots, \\theta_K\\}, \\pi) &= \\sum_{k=1}^K \\pi_k f(X_n | \\theta_k)\n\\end{align}\\] independently for each \\(n = 1, \\ldots, N\\). Here, \\(f(\\cdot)\\) is a probability density function such as the Gaussian density function or the categorical density function, and each component has its own weight, \\(\\pi_k\\), and set of parameters, \\(\\theta_k\\). The component weights are restricted to the unit simplex, i.e., \\(\\sum_{k=1}^K \\pi_k = 1\\). To capture the discrete structure in the data, we introduce an allocation variable, \\(c=[c_1, \\ldots, c_N]^\\top, c_n \\in [1, K] \\subset \\mathbb{N}\\), to indicate which component a sample was drawn from, introducing conditional independence between the components, \\[\\begin{align}\n    p(c_n = k) &= \\pi_k, \\\\\n    p(X_n | c_n = k, \\theta_k) &= f(X_n | \\theta_k).\n\\end{align}\\] The joint model can then be written \\[\\begin{align}\n    p(X, c, K, \\pi, \\theta) &= p(X | c, \\pi, K, \\theta) p(\\theta | c, \\pi, K) p(c | \\pi, K) p(\\pi | K) p(K).\n\\end{align}\\] We assume conditional independence between certain parameters such that the model reduces to \\[\\begin{align}\n    p(X, c, \\theta, \\pi, K) &=  p(\\pi | K) p(\\theta | K) p(K) \\prod_{n=1}^N p(X_n | c_n, \\theta_{c_n}) p (c_n | \\pi).  \\label{eqn:jointMixModel}\n\\end{align}\\] In terms of the hierarchical model this is: \\[\\begin{align}\n    X_n | c_n, \\theta &\\sim F(\\theta_{c_n}), \\\\\n    c_n | \\pi &\\sim \\text{Multinomial}(\\pi_1, \\ldots, \\pi_K), \\\\\n    \\pi_1, \\ldots, \\pi_K &\\sim \\text{Dirichlet}(\\alpha / K, \\ldots, \\alpha / K), \\\\\n    \\theta_k &\\sim G^{(0)},\n\\end{align}\\] where \\(F\\) is the appropriate distribution (e.g., Gaussian, Categorical, etc.) and \\(G^{(0)}\\) is some prior over the component parameters."
  },
  {
    "objectID": "posts/mdir/mdir.html#mdi",
    "href": "posts/mdir/mdir.html#mdi",
    "title": "mdir: Bayesian multi-omics clustering",
    "section": "MDI",
    "text": "MDI\nMDI is a Bayesian integrative or multi-modal clustering method. Signal sharing is defined by the prior on the cluster label of the \\(n^{th}\\) observation in the \\(M\\) modalities: \\[\\begin{align}\np(c_n^{(1)}, \\ldots, c_n^{(M)} | \\cdot) &= \\prod_{m=1}^M \\gamma^{(m)}_{c_n^{(m)}} \\prod_{m = 1}^{M-1} \\prod_{l = m + 1}^M (1 + \\phi_{(m, l)} \\mathbb{I} (c_n^{(m)} = c_n^{(l)})),\n\\end{align}\\] where \\(c_n^{(m)}\\) is the label of the \\(n^{th}\\) observation in the \\(m^{th}\\) modality, \\(\\gamma^{(m)}_{k}\\) is the weight of the \\(k^{th}\\) cluster in the \\(m^{th}\\) modality, \\(\\phi_{(m, l)}\\) is the similarity between the clusterings of the \\(m^{th}\\) and \\(l^{th}\\) modalities and \\(\\mathbb{I}(x)\\) is the indicator function returning 1 if \\(x\\) is true and 0 otherwise. Attractively, \\(\\phi_{(m, l)}\\) is inferred from the data and if there is no shared signal it will tend towards 0 giving us no dependency between these modalities. Thus each dataset will have independent clusters."
  },
  {
    "objectID": "posts/Canada/index.html",
    "href": "posts/Canada/index.html",
    "title": "Montreal - BAYSM and ISBA",
    "section": "",
    "text": "I attended the BAyesian Young Statisticians Meeting (BAYSM) and the International Society for Bayesian Analysis in Montreal in June.\nBAYSM was great, despite intense jet lag, though the keynotes were three people who have all made significant contributions to (among other things) model-based clustering: Adrian Raftery, Sylvia Frühwirth-Schnatter and Amy Herring. Also, a number of people I’d met at the BNP meeting in Cyprus were in attendence which was a delight. This was two days and less than 100 people. ISBA was a much more intense affair. I think about 600 people attended in the end? It lasted the week and had very long and full days. Though I also only started sleeping correctly in the last two days, so my sleep deficit was pretty strong, so I might be overstating it."
  },
  {
    "objectID": "posts/Canada/index.html#baysm",
    "href": "posts/Canada/index.html#baysm",
    "title": "Montreal - BAYSM and ISBA",
    "section": "BAYSM",
    "text": "BAYSM\nI really recommend BAYSM to any junior Bayesians. It was very well done and most of the speakers had clearly thought about making their work accessible so I enjoyed many of the talks which covered quite an array of subjects. I also spoke myself about my batch-correction method, so that was fun. I think my talk went down pretty well, certainly I got laughs at all the spots I had hoped to (and, more importantly, none of the spots I hadn’t) and had some questions from both Amy and Sylvia which was cool. Jason Roy, the chair of my session, also some nice questions.\nFrom the other talks the two standouts for me were from Bella Deutsch and Renato Berlinghieri. Bella made me feel like an understood Hawke’s Processes and Approximate Bayesian Computation for a few hours, her paper is on arXiv [@Deutsch2020ABCHawkes]. Renato had delightful slides and some very nice material in his talk about using Gaussian processes to model ocean currents."
  },
  {
    "objectID": "posts/Canada/index.html#isba",
    "href": "posts/Canada/index.html#isba",
    "title": "Montreal - BAYSM and ISBA",
    "section": "ISBA",
    "text": "ISBA\nI had a poster covering the same material as my BAYSM talk. My memory of ISBA is limited at this point, but some highlights include conversations with Saif Syed and Brian Trippe and a jazz evening where Jordan Bryan and Dennis Niemann astounded us with their musicality."
  },
  {
    "objectID": "posts/batchmix/index.html",
    "href": "posts/batchmix/index.html",
    "title": "batchmix: Bayesian mixture modelling for multi-batch data",
    "section": "",
    "text": "My R package, batchmix, finally landed on CRAN. This implements the models described in my paper (Coleman et al. 2022), mixture models aimed at accounting for batch effects.\nThe model is very similar to the ComBat algorithm of Johnson, Li, and Rabinovic (2007), but infers batch effects and class at the same time. This makes the method semi-supervised as it uses the inferred labels to update class parameters. This offers some protection from unequal representation of class across batches which is nice.\nThis post will essentially recreate the vignette for the package showing how to use it."
  },
  {
    "objectID": "posts/batchmix/index.html#data-generation",
    "href": "posts/batchmix/index.html#data-generation",
    "title": "batchmix: Bayesian mixture modelling for multi-batch data",
    "section": "Data generation",
    "text": "Data generation\nWe simulate some data using the generateBatchData function.\n\nlibrary(ggplot2)\nlibrary(batchmix)\nlibrary(mdiHelpR)\n\n\nAttaching package: 'mdiHelpR'\n\n\nThe following object is masked from 'package:batchmix':\n\n    createSimilarityMat\n\n\nThe following object is masked from 'package:methods':\n\n    show\n\nset.seed(1)\nsetMyTheme()\n\n# Data dimensions\nN <- 600\nP <- 4\nK <- 5\nB <- 7\n\n# Generating model parameters\nmean_dist <- 2.25\nbatch_dist <- 0.3\ngroup_means <- seq(1, K) * mean_dist\nbatch_shift <- rnorm(B, mean = batch_dist, sd = batch_dist)\nstd_dev <- rep(2, K)\nbatch_var <- rep(1.2, B)\ngroup_weights <- rep(1 / K, K)\nbatch_weights <- rep(1 / B, B)\ndfs <- c(5, 10, 15, 20, 80)\n\nmy_data <- generateBatchData(\n  N,\n  P,\n  group_means,\n  std_dev,\n  batch_shift,\n  batch_var,\n  group_weights,\n  batch_weights,\n  type = \"MVT\",\n  group_dfs = dfs\n)\n\nThis gives us a named list with two related datasets, the observed_data which includes batch effects and the corrected_data which is batch-free. It also includes group_IDs, a vector indicating class membership for each item, batch_IDs, which indicates batch of origin for each item, and fixed, which indicates which labels are observed and fixed in the model. We pull these out of the names list in the format that the modelling functions desire them.\n\nX <- my_data$observed_data\n\ntrue_labels <- my_data$group_IDs\nfixed <- my_data$fixed\nbatch_vec <- my_data$batch_IDs\n\nalpha <- 1\ninitial_labels <- generateInitialLabels(alpha, K, fixed, true_labels)"
  },
  {
    "objectID": "posts/batchmix/index.html#modelling",
    "href": "posts/batchmix/index.html#modelling",
    "title": "batchmix: Bayesian mixture modelling for multi-batch data",
    "section": "Modelling",
    "text": "Modelling\nGiven some data, we are interested in modelling it. We assume here that the set of observed labels includes at least one example of each class in the data.\n\n# Sampling parameters\nR <- 1000\nthin <- 50\nn_chains <- 4\n\n# Density choice\ntype <- \"MVT\"\n\n# MCMC samples and BIC vector\nmcmc_output <- runMCMCChains(\n  X,\n  n_chains,\n  R,\n  thin,\n  batch_vec,\n  type,\n  initial_labels = initial_labels,\n  fixed = fixed\n)\n\nWe want to assess two things. First, how frequently the proposed parameters in the Metropolis-Hastings step are accepted:\n\nplotAcceptanceRates(mcmc_output)\n\n\n\n\nSecondly, we want to asses how well our chains have converged. To do this we plot the complete_likelihood of each chain. This is the quantity most relevant to a clustering/classification, being dependent on the labels. The observed_likelihood is independent of labels and more relevant for density estimation.\n\nplotLikelihoods(mcmc_output)\n\n\n\n\nWe see that our chains disagree. We have to run them for more iterations. We use the continueChains function for this.\n\nR_new <- 9000\n\n# Given an initial value for the parameters\nnew_output <- continueChains(\n  mcmc_output,\n  X,\n  fixed,\n  batch_vec,\n  R_new,\n  keep_old_samples = TRUE\n)\n\nTo see if the chains better agree we re-plot the likelihood.\n\nplotLikelihoods(new_output)\n\n\n\n\nWe also re-check the acceptance rates.\n\nplotAcceptanceRates(new_output)\n\n\n\n\nThis looks like several of the chains agree by the 5,000th iteration."
  },
  {
    "objectID": "posts/batchmix/index.html#process-chains",
    "href": "posts/batchmix/index.html#process-chains",
    "title": "batchmix: Bayesian mixture modelling for multi-batch data",
    "section": "Process chains",
    "text": "Process chains\nWe process the chains, acquiring point estimates of different quantities.\n\n# Burn in\nburn <- 5000\n\n# Process the MCMC samples\nprocessed_samples <- processMCMCChains(new_output, burn)"
  },
  {
    "objectID": "posts/batchmix/index.html#visualisation",
    "href": "posts/batchmix/index.html#visualisation",
    "title": "batchmix: Bayesian mixture modelling for multi-batch data",
    "section": "Visualisation",
    "text": "Visualisation\nFor multidimensional data we use a PCA plot.\n\nchain_used <- processed_samples[[1]]\n\npc <- prcomp(X, scale = T)\npc_batch_corrected <- prcomp(chain_used$inferred_dataset)\n\nplot_df <- data.frame(\n  PC1 = pc$x[, 1],\n  PC2 = pc$x[, 2],\n  PC1_bf = pc_batch_corrected$x[, 1],\n  PC2_bf = pc_batch_corrected$x[, 2],\n  pred_labels = factor(chain_used$pred),\n  true_labels = factor(true_labels),\n  prob = chain_used$prob,\n  batch = factor(batch_vec)\n)\n\nplot_df |> \n  ggplot(aes(\n    x = PC1,\n    y = PC2,\n    colour = true_labels,\n    alpha = prob\n  )) +\n  geom_point()\n\n\n\nplot_df |> \n  ggplot(aes(\n    x = PC1_bf,\n    y = PC2_bf,\n    colour = pred_labels,\n    alpha = prob\n  )) +\n  geom_point()\n\n\n\ntest_inds <- which(fixed == 0)\n\nsum(true_labels[test_inds] == chain_used$pred[test_inds])/length(test_inds)\n\n[1] 0.8404255"
  },
  {
    "objectID": "posts/ClusteringCountData/clustering_count_data.html",
    "href": "posts/ClusteringCountData/clustering_count_data.html",
    "title": "Clustering count data",
    "section": "",
    "text": "This vignette explores clustering count data and the impact of some data transforms on this.\nWe will consider a \\(\\log\\)-transform and the mean-centring and scaling (i.e., standardisation). For a vector of data \\(X\\), these are:\n\\[\\begin{align}\n\\text{Log-transform: } X \\to \\log(1 + X), \\\\\n\\text{Standardisation: } X \\to \\frac{X - \\bar{X}}{\\bar{\\sigma}},\n\\end{align}\\]\nwhere \\(\\bar{X}\\) is the empirical mean of \\(X\\), and \\(\\bar{\\sigma}\\) is the empirical standard deviation."
  },
  {
    "objectID": "posts/ClusteringCountData/clustering_count_data.html#generate-count-data",
    "href": "posts/ClusteringCountData/clustering_count_data.html#generate-count-data",
    "title": "Clustering count data",
    "section": "Generate count data",
    "text": "Generate count data\nFirst we create 5 subpopulations with some peturbation about a mean.\n\n# Example of transforms on poission data\nn <- 100\nbeta_0 <- c(1, 1, 5, 6, 2, 2)\nbeta_1 <- c(0.2, 0.2, 1, 2.5, 2, 2)\n\n# Generate random data\nx <- runif(n = n, min = 0, max = 2.0)\n\n# Generate data from 4 poisson regression models\nbeta_1_mat <- sapply(beta_1, `*`, x)\nexponent_mat <- t(apply(beta_1_mat, 1, `+`, t(beta_0)))\n\npoisson_data <- apply(exp(exponent_mat), 2, function(x) {\n  rpois(n = n, lambda = x)\n})\n\n# Put this data into a data.frame\npoisson_df <- data.frame(\n  Count_1 = c(poisson_data[, 1], poisson_data[, 3]),\n  Count_2 = c(poisson_data[, 2], poisson_data[, 4]),\n  Count_3 = c(poisson_data[, 5], poisson_data[, 6])\n)\n\nCreate some data that follows a sigmoidal curve:\n\n# some continuous variables\nx1 <- runif(2 * n, -2, -1)\nx2 <- runif(2 * n, 1, 2)\nx3 <- runif(2 * n, -1, 1)\n\n# linear combination with a bias\nz <- 1 + 8 * x1 + 7.5 * x2 + 5 * x3\n\n# pass through an inverse logit function and move to a scale similar to a count\npr <- round(1 / (1 + exp(-z)) * 1000)\n\nplot(z, pr, main = \"Discere sigmoidal data\")\n\n\n\nhist(pr, main = \"Discere sigmoidal data\")\n\n\n\n\nWe add a feature generated from a different model; this follows a sigmoidal curve. We combine this with our previously generated data.\n\n# Arbitrary step to have the high counts align in each dataset imperfectly but well\n# enough to have less sub-populations emerge from the combined dataset\nfractions <- 8\nnew_order <- order(pr)\nflag <- as.numeric(cut(pr,\n  breaks = quantile(pr, probs = seq(0, 1, 1 / fractions)),\n  include.lowest = T,\n  labels = 1:fractions\n))\n\nflag[flag %in% 3:4] <- fractions + 1\n\n# Combine the generated data\nmy_data <- as.data.frame(cbind(poisson_df, pr[order(flag)]))\n\n# Assign row and column names\ncolnames(my_data) <- c(paste0(\"Count_\", 1:(ncol(poisson_df) + 1)))\nrow.names(my_data) <- paste(\"Person_\", 1:nrow(my_data))\n\n# Of some use later\nn_var <- ncol(my_data)\n\nhead(my_data)\n\n          Count_1 Count_2 Count_3 Count_4\nPerson_ 1       4       2      18       0\nPerson_ 2       2       2      30       3\nPerson_ 3       2       3      89      11\nPerson_ 4       9       3     269       0\nPerson_ 5       3       1      23       2\nPerson_ 6       2       4     238       0\n\n\nNow we apply our transforms.\n\n# Log transform\nlog_data <- log(1 + my_data) %>% as.data.frame()\n\n# Mean centre and standardise the standard deviation within each variable\nscaled_data <- apply(my_data, 2, scale) %>%\n  as.data.frame() |>\n  set_rownames(row.names(my_data))\n\n# Let's now try combining these two\nscaled_log_data <- apply(log(1 + my_data), 2, scale) %>%\n  as.data.frame() |>\n  set_rownames(row.names(my_data))\n\nLet us look at the distributions described by each variable for each dataset. We expect there to be two subpopulations present under variables “Count_1”, “Count_2” and “Count_4”, and a single population under “Count_3”.\n\nggplot(gather(my_data), aes(value)) +\n  geom_histogram() +\n  facet_wrap(~key, scales = \"free_x\") +\n  labs(\n    title = \"Distribution of original data\",\n    x = \"Count\",\n    y = \"Frequency\"\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\npheatmap(my_data,\n  color = col_pal,\n  main = \"Generated data\",\n  show_rownames = FALSE,\n  cluster_cols = FALSE\n)\n\n\n\nggplot(gather(log_data), aes(value)) +\n  geom_histogram() +\n  facet_wrap(~key, scales = \"free_x\") +\n  labs(\n    title = \"Distribution of log-transformed data\",\n    x = \"Count\",\n    y = \"Frequency\"\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\npheatmap(log_data,\n  color = col_pal,\n  main = \"Log-transformed data\",\n  show_rownames = FALSE,\n  cluster_cols = FALSE\n)\n\n\n\nggplot(gather(scaled_data), aes(value)) +\n  geom_histogram() +\n  facet_wrap(~key, scales = \"free_x\") +\n  labs(\n    title = \"Distribution of standardised data\",\n    x = \"Count\",\n    y = \"Frequency\"\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\npheatmap(scaled_data,\n  color = col_pal,\n  main = \"Standardised data\",\n  show_rownames = FALSE,\n  cluster_cols = FALSE\n)\n\n\n\nggplot(gather(scaled_log_data), aes(value)) +\n  geom_histogram() +\n  facet_wrap(~key, scales = \"free_x\") +\n  labs(\n    title = \"Distribution of standardised log-transformed data\",\n    x = \"Count\",\n    y = \"Frequency\"\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\npheatmap(scaled_log_data,\n  color = col_pal,\n  main = \"Standardised log-transformed data\",\n  show_rownames = FALSE,\n  cluster_cols = FALSE\n)\n\n\n\n\nNote that standardising has not impacted how well-separated the groups in the data are, it has merely changed the scale and location of the data.\nIn terms of separating out the subpopulations, it appears that the \\(\\log\\)-transform has worked most effectively for the first two variables, but does not succeed with the sigmoidal data. This reminds us that a single transform might not be appropriate for an entire dataset; however, many datasets are too large to check each feature manually, this is simply to make the point that we often lose some signal and minimising this is only so feasible.\nWe will now attempt to infer the latent clustering labels using mixture models. I use the Mclust function from the mclust package and the mvnormalmixEM from mixtools to create Gaussian mixture models. We will look at the clustering predicted for the data using pheatmap.\nI create some labels for the sigmoidal data to keep track of the tails - we hope these are allocated correctly and are probably the hardest sub-populations to untangle for the \\(\\log\\)-transformed data. These labels are based on which tertiles the sigmoidal data falls into and is intended as a rough guide of how well the models deconstruct the sigmoidal data.\n\n# Keep track of the sigmoidal data by assigning a label based on quantiles\nn_labels <- 3\nsig_labels <- cut(my_data[, n_var],\n  breaks = quantile(my_data[, n_var], probs = seq(0, 1, 1 / n_labels)),\n  include.lowest = T,\n  labels = 1:n_labels\n)\n\n# Create a data.frame to annotate the heatmaps\nannotation_row <- data.frame(Sig_pop = as.factor(sig_labels))\nrow.names(annotation_row) <- row.names(my_data)"
  },
  {
    "objectID": "posts/ClusteringCountData/clustering_count_data.html#model-fitting",
    "href": "posts/ClusteringCountData/clustering_count_data.html#model-fitting",
    "title": "Clustering count data",
    "section": "Model fitting",
    "text": "Model fitting\nNow attempt to fit models. I find that mixtools is less robust to mclust (struggles to solve the same datasets, even with the hclust initialisation). For this reason I comment out the code for fitting the mixtools model.\n\nmodel_functions <- c(\n  \"Mclust\",\n  \"mvnormalmixEM\"\n)\n\ntransforms <- c(\n  \"Original\",\n  \"Log\",\n  \"Standardise\"\n)\n\ndatasets <- list(my_data, log_data, scaled_data) %>%\n  set_names(transforms)\n\nn_datasets <- length(datasets)\nn_models <- length(model_functions)\n\nmodel_out <- vector(\"list\", n_models) %>%\n  set_names(model_functions)\n\nmodel_bic <- model_out\n\nfor (i in 1:n_models) {\n  model_out[[i]] <- vector(\"list\", n_datasets) %>%\n    set_names(transforms)\n\n  model_bic[[i]] <- model_out[[i]]\n\n  if (model_functions[i] == \"Mclust\") {\n    for (j in 1:n_datasets) {\n      # do.call(model_functions[i], list(datasets[[j]])\n      model_out[[i]][[j]] <- Mclust(datasets[[j]], G = 2:15)\n      model_bic[[i]][[j]] <- mclustBIC(datasets[[j]])\n    }\n  }\n  # if (model_functions[i] == \"mvnormalmixEM\") {\n  #   for (j in 1:n_datasets) {\n  #     for(k in seq(2, 15)) {\n  #       initial_clusterings <- datasets[[j]] |>\n  #         dist() |>\n  #         hclust() |>\n  #         cutree(k = k)\n  #       initial_means <- vector(\"list\", k)\n  #       for(l in seq(1, k)) {\n  #         cluster_inds <- which(initial_clusterings == l)\n  #         initial_means[[l]] <- colMeans(datasets[[j]][cluster_inds, ])\n  #       }\n  #\n  #       model_out[[i]][[j]] <- mvnormalmixEM(datasets[[j]], k = k, mu = initial_means)\n  #     }\n  #   }\n  # }\n}\n\nWe can now inspect the model using several different visualisations. We can investigate the optimal number of components under the Bayesian Information Criterion (BIC) and we can look at the clusterings as defined by pairs of variables. I will look at the models defined on the scaled data and the log-transformed data. The BIC vs \\(k\\) plot also shows which of the possible types of covariance structure allowed by Mclust is optimal (this is the difference models listed and plotted).\n\nsummary(model_out[[1]][[2]])\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust VVE (ellipsoidal, equal orientation) model with 8 components: \n\n log-likelihood   n df       BIC       ICL\n      -456.7326 200 77 -1321.436 -1331.303\n\nClustering table:\n 1  2  3  4  5  6  7  8 \n21 29 30 20 49 17  7 27 \n\nplot(model_out[[1]][[2]], what = \"classification\")\n\n\n\nplot(model_out[[1]][[2]], what = \"BIC\")\n\n\n\nsummary(model_out[[1]][[3]])\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust VVV (ellipsoidal, varying volume, shape, and orientation) model with 8\ncomponents: \n\n log-likelihood   n  df      BIC      ICL\n       990.4389 200 119 1350.378 1344.508\n\nClustering table:\n 1  2  3  4  5  6  7  8 \n20 30 25 25 23 24 23 30 \n\nplot(model_out[[1]][[3]], what = \"classification\")\n\n\n\nplot(model_out[[1]][[3]], what = \"BIC\")\n\n\n\n\nWe can see that by using a vector of possible components (the input G = 2:15 in the call to Mclust), we have captured the optimal value. This is the global maximum in the plot comparing BIC to number of components for each model allowed by Mclust. One thing to notice here is that the two models do agree on the optimal number of components (8), but that this is not expected to occurr, particularly for datasets of greater dimension. Another point is that the type of model that best fits the scaled data is the “EVE” model in comparison to the “VVV” model. This is a simpler model where each cluster has more restrictions on its parameters - thus the EVE model is easier to run (this is worth remembering if log-transformed data demands to complex a model computationally).\nIn the plots comparing clusterings across variables, we can see that the model defined on the log-transformed data separates the two sub-populations in \\(Count_1\\) and \\(Count_2\\) with much greater confidence than the model defined on the standardised data. One can also see that \\(Count_3\\), which has no sub-population structure, is not contributing significantly to the cluster allocaitons - the data is pretty uniformly distributed across the axis defined by \\(Count_3\\), no clear partitions emerging. One can see that the sigmoidal structure in \\(Count_4\\) is captured quie well by the log model."
  },
  {
    "objectID": "posts/ClusteringCountData/clustering_count_data.html#results",
    "href": "posts/ClusteringCountData/clustering_count_data.html#results",
    "title": "Clustering count data",
    "section": "Results",
    "text": "Results\nLet us now inspect the clustering inferred I ignore the original data as the scaling makes inspecting the data unfeasible.\n\nlabelling <- lapply(model_out$Mclust, function(x) {\n  x$classification\n}) |>\n  as.data.frame() |>\n  set_rownames(row.names(my_data))\n\nannotation_row <- labelling\n\npheatmap(log_data[order(annotation_row[, 1]), ],\n  cluster_rows = F,\n  cluster_cols = F,\n  annotation_row = annotation_row,\n  main = \"Log-transformed data:\\nOrdered by clustering of original data\"\n)\n\n\n\npheatmap(log_data[order(annotation_row[, 2]), ],\n  cluster_rows = F,\n  cluster_cols = F,\n  annotation_row = annotation_row,\n  main = \"Log-transformed data:\\nOrdered by clustering of log-transformed data\"\n)\n\n\n\npheatmap(log_data[order(annotation_row[, 3]), ],\n  cluster_rows = F,\n  cluster_cols = F,\n  annotation_row = annotation_row,\n  main = \"Log-transformed data:\\nOrdered by clustering of scaled data\"\n)\n\n\n\npheatmap(scaled_data[order(annotation_row[, 1]), ],\n  cluster_rows = F,\n  cluster_cols = F,\n  annotation_row = annotation_row,\n  main = \"Standardised data:\\nOrdered by clustering of original data\"\n)\n\n\n\npheatmap(scaled_data[order(annotation_row[, 2]), ],\n  cluster_rows = F,\n  cluster_cols = F,\n  annotation_row = annotation_row,\n  main = \"Standardised data:\\nOrdered by clustering of log-transformed data\"\n)\n\n\n\npheatmap(scaled_data[order(annotation_row[, 3]), ],\n  cluster_rows = F,\n  cluster_cols = F,\n  annotation_row = annotation_row,\n  main = \"Standardised data:\\nOrdered by clustering of scaled data\"\n)\n\n\n\n\nWe comapre the similarity of the inferred clusterings using the Adjusted Rand Index. This scores clustering similarities, with 0 meaning the two partitions are no more similar then a random pair of clusterings is expected to be and 1 meaning they are identical.\n\nari_12 <- mcclust::arandi(annotation_row[, 1], annotation_row[, 2])\nari_13 <- mcclust::arandi(annotation_row[, 1], annotation_row[, 3])\nari_23 <- mcclust::arandi(annotation_row[, 2], annotation_row[, 3])\n\nari_mat <- matrix(c(1.0, ari_12, ari_13, ari_12, 1.0, ari_23, ari_13, ari_23, 1.0),\n  nrow = 3,\n  ncol = 3\n) |> \n  set_colnames(c(\"Original\", \"Log-transformed\", \"Standardised\")) |> \n  set_rownames(c(\"Original\", \"Log-transformed\", \"Standardised\"))\n\nknitr::kable(ari_mat, digits = 3)\n\n\n\n\n\nOriginal\nLog-transformed\nStandardised\n\n\n\n\nOriginal\n1.000\n0.702\n1.000\n\n\nLog-transformed\n0.702\n1.000\n0.702\n\n\nStandardised\n1.000\n0.702\n1.000"
  },
  {
    "objectID": "posts/ClusteringCountData/clustering_count_data.html#summary",
    "href": "posts/ClusteringCountData/clustering_count_data.html#summary",
    "title": "Clustering count data",
    "section": "Summary",
    "text": "Summary\nWe see that from the Gaussian mixture models perspective the original and standardised are interchangeable, leading to identical inference. Both datasets lead to a surprisingly similar clustering to the log-transformed data; 0.7 is not a low ARI. Furthermore, we can see in the annotated heatmaps that a large source of the contention is that one cluster found in the log-transformed data is considered as two separate groups in the original (cluster 6 in the log-transformed inference approximately captures clusters 6 and 5 from the other point estimates), and conversely cluster 8 in the original data splits into two clusters in the log-transformed data. Deciding which of these is more useful, or if we should use 7 or 9 clusters rather then 8 involves further thought and ideally conversation with a domain expert."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Stephen Coleman",
    "section": "",
    "text": "I work with Bayesian model-based clustering methods."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Updating Priors",
    "section": "",
    "text": "Circumventing poor mixing in Bayesian model-based clustering\n\n\n\n\n\n\n\nBayes\n\n\n\n\n\n\n\n\n\n\n\nStephen Coleman\n\n\n\n\n\n\n\n\nmdir: Bayesian multi-omics clustering\n\n\n\n\n\n\n\nBayes\n\n\n\n\n\n\n\n\n\n\n\nStephen Coleman\n\n\n\n\n\n\n\n\nMontreal - BAYSM and ISBA\n\n\n\n\n\n\n\nBayes\n\n\n\n\n\n\n\n\n\n\n\nStephen Coleman\n\n\n\n\n\n\n\n\nbatchmix: Bayesian mixture modelling for multi-batch data\n\n\n\n\n\n\n\nBayes\n\n\n\n\n\n\n\n\n\n\n\nStephen Coleman\n\n\n\n\n\n\n\n\nClustering count data\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nStephen Coleman\n\n\n\n\n\n\nNo matching items"
  }
]