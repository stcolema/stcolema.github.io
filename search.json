[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Me",
    "section": "",
    "text": "I am a PhD student in Cambridge working with Bayesian model-based clustering methods."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Barefoot Bayes",
    "section": "",
    "text": "Circumventing poor mixing in Bayesian model-based clustering\n\n\n\n\n\n\n\nBayes\n\n\n\n\n\n\n\n\n\n\n\nStephen Coleman\n\n\n\n\n\n\n\n\nCyprus\n\n\n\n\n\n\n\nBayes\n\n\ncoffee\n\n\nrunning\n\n\n\n\n\n\n\n\n\n\n\nStephen Coleman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/consensusClustering/consensus_clustering.html",
    "href": "posts/consensusClustering/consensus_clustering.html",
    "title": "Circumventing poor mixing in Bayesian model-based clustering",
    "section": "",
    "text": "Paper: Consensus Clustering for Bayesian Mixture Models\n\nAim: Perform inference on Bayesian clustering methods when chains do not converge without reimplementation\nPros: Easy to use and offers useful inference\nCons: Inference is no longer Bayesian"
  },
  {
    "objectID": "posts/consensusClustering/consensus_clustering.html#introduction",
    "href": "posts/consensusClustering/consensus_clustering.html#introduction",
    "title": "Circumventing poor mixing in Bayesian model-based clustering",
    "section": "Introduction",
    "text": "Introduction\nThe first paper of my PhD was entitled “Consensus Clustering for Bayesian Mixture Models” and written with my supervisors Paul and Chris. I think that the paper is a little confused, but it covers some nice work. To overcome the issue of poor mixing that haunts Bayesian clustering, we proposed a heuristic method based on the consensus clustering algorithm of Monti et al. (2003) that appears to enable principled inference without reimplementing the sampler. However, the inference does lose the Bayesian interpretation.\nIf you have attempted to infer discrete latent structure using a Bayesian method implemented using Markov Chain Monte Carlo (MCMC) methods, then you’ve probably encountered the problem of poor mixing where chains become trapped in different clusterings and stop exploring the parameter space.There is a large body of literature about methods for navigating this problem in a principled fashion (see, e.g., Dahl 2003; Jain and Neal 2004; Bouchard-Côté, Doucet, and Roth 2017; Syed et al. 2022), but these state-of-the-art samplers tend to be time-consuming to implement. In this paper we proposed taking samples from many (relatively) short chains rather than a single long chain. This idea has seen a lot of mileage recently across a few different papers, but where we differ is we surrender our claim of being truly Bayesian. We have to do this as, while it is possibly that our samples are drawn from the target density, they are unlikely to be weighted correctly. However, we found that the expected values from the distributions sampled this way tended to be correct even if the variance might be too large."
  },
  {
    "objectID": "posts/consensusClustering/consensus_clustering.html#algorithm",
    "href": "posts/consensusClustering/consensus_clustering.html#algorithm",
    "title": "Circumventing poor mixing in Bayesian model-based clustering",
    "section": "Algorithm",
    "text": "Algorithm\nThe basic method is sketched below:\n\nConsensus clustering for Bayesian methods\nInputs:\n  W: the number of chains to run\n  D: the number of iterations to run each chain for\n  sampler: the MCMC sampler for the method\n  priorDraw(): a method of drawing a sample from the prior distribution over the clustering space (this is often integrated into the sampler in practice)\n \nOutput:\n  c: a matrix of sampled partitions\n\nAlgorithm:\n  for(w in 1:W) {\n    # Differentiate the model runs using the random seed\n    set.seed(w);\n    c_0 <- prior();\n    for(r in 1:R) {\n      # Sample a new clustering based on the current partition\n      c_r <- sampler(c_r);\n    }\n    # Save the last iteration\n    c[w] <- c_r\n  }\n\nObviously one can sample more quantities than just the clustering (as we do in the paper), but I think that clustering works particularly well with short chains. This is because we often care about co-clustering probabilities across sampled partitions rather than any single sample being correct; this means that if there is enough variety across a large number of poor (but better than random) sampled partitions we might still infer the correct structure. For other variables we need longer chains to have the correct expected value from our sampled distribution."
  },
  {
    "objectID": "posts/consensusClustering/consensus_clustering.html#guessing-chain-length",
    "href": "posts/consensusClustering/consensus_clustering.html#guessing-chain-length",
    "title": "Circumventing poor mixing in Bayesian model-based clustering",
    "section": "Guessing chain length",
    "text": "Guessing chain length\nTo use consensus clustering, we want to have an idea of how long we will need to run the chains for. In this case we run a small number of chains for longer than necessary and the look at trace plots to see when they stopped exploring or the degree of exploration had tapered off significantly.\n\nn_initial_chains <- 9\nR_long <- 5000\nthin_long <- 50\niterations_initial <- seq(0, R_long, thin_long)\n\nn_samples <- floor(R_long / thin_long) + 1\ntype <- \"G\"\n\npsms <- initial_mcmc <- vector(\"list\", n_initial_chains)\nlikelihood_df <- matrix(0, nrow = n_samples - 1, ncol = n_initial_chains)\ncolnames(likelihood_df) <- paste0(\"Chain\", seq(1, n_initial_chains))\n\nari_mat <- concentration_df <- matrix(0, nrow = n_samples, ncol = n_initial_chains)\ncolnames(ari_mat) <- colnames(concentration_df) <- paste0(\"Chain\", seq(1, n_initial_chains))\n\nensemble_psm <- matrix(0, nrow = N, ncol = N)\n\nfor (ii in seq(1, n_initial_chains)) {\n  initial_mcmc[[ii]] <- .mcmc <- callMixtureModel(my_data, R_long, thin_long, type)\n  likelihood_df[, ii] <- .mcmc$complete_likelihood[-1]\n  concentration_df[, ii] <- .mcmc$mass\n  ari_mat[, ii] <- apply(.mcmc$allocations, 1, function(x) {mcclust::arandi(x, .mcmc$allocations[n_samples, ])})\n  .psm <- makePSM(initial_mcmc[[ii]]$allocations)\n  row.names(.psm) <- colnames(.psm) <- row.names(my_data)\n  psms[[ii]] <- .psm\n  ensemble_psm <- ensemble_psm + .psm\n}\n\nensemble_psm <- ensemble_psm / n_initial_chains\n\n\nari_df <- ari_mat |> \n  as.data.frame() |> \n  mutate(Iteration = iterations_initial) |>\n  pivot_longer(-Iteration, names_to = \"Chain\", values_to = \"ARI\")\n\nlikelihood_df2 <- likelihood_df |>\n  as.data.frame() |>\n  mutate(Iteration = iterations_initial[-1]) |>\n  pivot_longer(-Iteration, names_to = \"Chain\", values_to = \"Complete_log_likelihood\")\n\nmass_df <- concentration_df |>\n  as.data.frame() |>\n  mutate(Iteration = iterations_initial) |>\n  pivot_longer(-Iteration, names_to = \"Chain\", values_to = \"Mass\")\n\nlikelihood_df2 |>\n  ggplot(aes(x = Iteration, y = Complete_log_likelihood, group = Chain)) +\n  geom_line() +\n  ggthemes::scale_color_colorblind()\n\n\n\nmass_df |>\n  ggplot(aes(x = Iteration, y = Mass, group = Chain)) +\n  geom_line() +\n  ggthemes::scale_color_colorblind()\n\n\n\nari_df |> \n  ggplot(aes(x = Iteration, y = ARI, group = Chain)) +\n  geom_line() +\n  ggthemes::scale_color_colorblind() +\n  labs(title = \"ARI between rth sampled partition and final sampled partition in each chains\")\n\n\n\npsm <- makePSM(.mcmc$allocations)\nrow.names(psm) <- colnames(psm) <- row.names(my_data)\n\n# Create the heatmap of the PSM\npheatmap::pheatmap(psm,\n  color = simColPal(),\n  breaks = defineBreaks(simColPal(), lb = 0),\n  annotation_row = anno_row,\n  annotation_colors = ann_colours,\n  main = \"Example PSM annotated by clusterings\",\n  show_rownames = FALSE,\n  show_colnames = FALSE\n)\n\n\n\npsm_df <- prepSimilarityMatricesForGGplot(psms)\n\npsm_df |>\n  ggplot(aes(x = x, y = y, fill = Entry)) +\n  geom_tile() +\n  facet_wrap(~Chain) +\n  scale_fill_gradient(\n    low = \"#FFFFFF\",\n    high = \"#146EB4\"\n  ) +\n  labs(title = \"PSMs for initial chains with common ordering\")\n\n\n\n\nThese long chains have stopped exploring new clusterings almost instantly. The mass parameter takes awhile to converge, but this is sampled via Metropolis-Hastings, so this makes sense.\nThese PSMs also show why nested \\(\\hat{R}\\) and coupling don’t work for clustering. These methods assume that the different chains can eventually reach similar places; this is unlikely to happen in any reasonable length of time in even as low-dimensional an example as this (70 features; hardly low-dimensional in many settings, but that’s one of the joys of ’omics)."
  },
  {
    "objectID": "posts/consensusClustering/consensus_clustering.html#depth-tests",
    "href": "posts/consensusClustering/consensus_clustering.html#depth-tests",
    "title": "Circumventing poor mixing in Bayesian model-based clustering",
    "section": "Depth tests",
    "text": "Depth tests\nEssentially we will follow the logic of Geweke and compare the distribution using early samples to later samples. We differ in that the two sets of samples are collected across many chains, but I think the logic holds. Here we use a t test to test if the sampled distributions of the mass parameter for the clustering and the complete log-likelihood are the same in earlier samples to the same distributions for the final sample from all the chains.\n\nm1 <- mass_df$Mass[mass_df$D == 100]\nm2 <- mass_df$Mass[mass_df$D == 300]\nm3 <- mass_df$Mass[mass_df$D == 600]\nm4 <- mass_df$Mass[mass_df$D == 1000]\n\nl1 <- lkl_df$Complete_log_likelihood[lkl_df$D == 100]\nl2 <- lkl_df$Complete_log_likelihood[lkl_df$D == 300]\nl3 <- lkl_df$Complete_log_likelihood[lkl_df$D == 600]\nl4 <- lkl_df$Complete_log_likelihood[lkl_df$D == 1000]\n\nt_d1_m_1 <- t.test(m1, m4)\nt_d1_m_2 <- t.test(m2, m4)\nt_d1_m_3 <- t.test(m3, m4)\n\nt_d1_l_1 <- t.test(l1, l4)\nt_d1_l_2 <- t.test(l2, l4)\nt_d1_l_3 <- t.test(l3, l4)"
  },
  {
    "objectID": "posts/consensusClustering/consensus_clustering.html#width-tests",
    "href": "posts/consensusClustering/consensus_clustering.html#width-tests",
    "title": "Circumventing poor mixing in Bayesian model-based clustering",
    "section": "Width tests",
    "text": "Width tests\nTesting if we are using a sufficient quantity of chains is harder as the chains are not ordered. I believe we can essentially copy the Geweke test again but compare disjoint subsets of samples. For example, take the final sample from a random two fifths of the chains and compare the distribution for these to the distribution from the remaining three fifths and then repeat this multiple times.\n\nfrac_used <- 0.4\ninds1 <- sample(seq(1, W), size = W * frac_used, replace = FALSE)\ninds2 <- sample(seq(1, W), size = W * frac_used, replace = FALSE)\ninds3 <- sample(seq(1, W), size = W * frac_used, replace = FALSE)\ninds4 <- sample(seq(1, W), size = W * frac_used, replace = FALSE)\ninds5 <- sample(seq(1, W), size = W * frac_used, replace = FALSE)\n\nt_w1_m <- t.test(m3[inds1], m3[-inds1])\nt_w1_l <- t.test(l3[inds1], l3[-inds1])\n\nt_w2_m <- t.test(m3[inds2], m3[-inds2])\nt_w2_l <- t.test(l3[inds2], l3[-inds2])\n\nt_w3_m <- t.test(m3[inds3], m3[-inds3])\nt_w3_l <- t.test(l3[inds3], l3[-inds3])\n\nt_w4_m <- t.test(m3[inds4], m3[-inds4])\nt_w4_l <- t.test(l3[inds4], l3[-inds4])\n\nt_w5_m <- t.test(m3[inds5], m3[-inds5])\nt_w5_l <- t.test(l3[inds5], l3[-inds5])\n\n\ntest_df <- data.frame(\n  \"D\" = c(100, 300, 600, D, D, D, D, D),\n  \"W\" = c(W, W, W, 0.2 * W, 0.2 * W, 0.2 * W, 0.2 * W, 0.2 * W),\n  \"Mass\" = c(\n    t_d1_m_1$p.value,\n    t_d1_m_2$p.value,\n    t_d1_m_3$p.value,\n    t_w1_m$p.value,\n    t_w2_m$p.value,\n    t_w3_m$p.value,\n    t_w4_m$p.value,\n    t_w5_m$p.value\n  ),\n  \"Complete_log_likelihood\" = c(\n    t_d1_l_1$p.value,\n    t_d1_l_2$p.value,\n    t_d1_l_3$p.value,\n    t_w1_l$p.value,\n    t_w2_l$p.value,\n    t_w3_l$p.value,\n    t_w4_l$p.value,\n    t_w5_l$p.value\n  )\n)\n\n# test_df$Mass <- round(test_df$Mass, 4)\n# test_df$Complete_log_likelihood <- round(test_df$Complete_log_likelihood, 4)\n\nNow the p values for the mass parameter and complete log-likelihood t-tests are shown in the below table.\n\nknitr::kable(test_df, caption = \"Convergence tests for consensus clustering\", digits = 3)\n\n\nConvergence tests for consensus clustering\n\n\nD\nW\nMass\nComplete_log_likelihood\n\n\n\n\n100\n500\n0.000\n0.508\n\n\n300\n500\n0.000\n0.810\n\n\n600\n500\n0.651\n0.931\n\n\n1000\n100\n0.630\n0.446\n\n\n1000\n100\n0.137\n0.395\n\n\n1000\n100\n0.485\n0.627\n\n\n1000\n100\n0.328\n0.466\n\n\n1000\n100\n0.217\n0.067\n\n\n\n\n\nThe mass is sampled via Metropolis Hastings, so it taking longer to converge than the complete log-likelihood (which is a direct function of the clustering) is not shocking."
  },
  {
    "objectID": "posts/consensusClustering/consensus_clustering.html#mean-abolute-difference-between-consensus-matrices",
    "href": "posts/consensusClustering/consensus_clustering.html#mean-abolute-difference-between-consensus-matrices",
    "title": "Circumventing poor mixing in Bayesian model-based clustering",
    "section": "Mean abolute difference between consensus matrices",
    "text": "Mean abolute difference between consensus matrices\nIn the paper I recommended using the below kind of plots, but these need more models for assessing as with only three levels of both the iteration and number of chains used we only have two data points for comparisons, so they have some limitations.\n\nmean_abs_diff_df <- makeCMComparisonSummaryDF(cms, models)\n\nmean_abs_diff_df |>\n  dplyr::filter(Quantity_varied == \"Depth\") |>\n  ggplot(aes(x = Depth, y = Mean_absolute_difference, color = factor(Width))) +\n  geom_line() +\n  labs(title = \"Assessing stability across cms for increasing chain depth\") +\n  ggthemes::scale_color_colorblind()\n\n\n\nmean_abs_diff_df |>\n  dplyr::filter(Quantity_varied == \"Width\") |>\n  ggplot(aes(x = Width, y = Mean_absolute_difference, color = factor(Depth))) +\n  geom_line() +\n  labs(title = \"Assessing stability across cms for increasing numbers of chains\") +\n  ggthemes::scale_color_colorblind()"
  },
  {
    "objectID": "posts/Cyprus/index.html",
    "href": "posts/Cyprus/index.html",
    "title": "Cyprus",
    "section": "",
    "text": "I attended the Bayesian Non-Parametrics Networking Event in Nicosia, Cyprus and then wandered around the interior of the island. An immense time.\nNotes:\n\nThe BNP commmunity are very friendly depsite the intimdation factor of David Dunson, Aad van der Vaart and Judith Rousseau; I will attend as many of BNP events as possible in the future as is possible.\nCyprus is great and has more variety than I expect, but I’m very glad I wasn’t there even two weeks later, too much sun for me.\nRome2Rio is a fantastic app and made the trip into the interior feasible."
  },
  {
    "objectID": "posts/Cyprus/index.html#background",
    "href": "posts/Cyprus/index.html#background",
    "title": "Cyprus",
    "section": "Background",
    "text": "Background\nI was delighted/massively intimidated to be offered a speaking slot at the BNP Networking Event in Cyprus in April 2022. I was delighted as Cyprus was meant to be great, but intimidated as my work wasn’t really non-parametric or Bayesian and I knew from the agenda that Aad van der Vaart and Judith Rousseau would be there; two people who have provided immensely important and innovative contributions to Bayesian non-parametrics. David Dunson would also be going, but I figured David was likely to appreciate my work on pragmatic grounds, but stil, I was not without additional trepidation due to this factor - I’m a big fan of David’s work, he’s very smart and rejection would have been uncomfortable to say the least.\nRegardless, I was going and decided to try and do something I’d wanted to do for awhile, a running holiday. I planned to get a bus from Nicosa into the interior and basically circumvent the Cypriot Mt. Olympus (not to be confused with the Grecian or Martian mountains of the same name). So even if I had a crippling rejection from all of the academics I most admired I would probably have a nice time of it. My old Vibrams were wearing thin, so I presented myself with a pair of the new V-trail 2.0 as I expected to be off-piste for a large chunk of the running. I was a little anxious that I hadn’t broken them in enough as I expected to be going serious distance a day wearing a backpack; I had only done about 40km before heading out.\nI also made sure to check out speciality coffee bars in Nicosa using the European Coffee Trip. I was determined to hit “A \\(\\kappa\\chi\\)offee project” at the least.\nThus, a holiday of Bayes, barefoot running and coffee was born."
  },
  {
    "objectID": "posts/Cyprus/index.html#nicosia-bnp-and-a-fantastic-coffee",
    "href": "posts/Cyprus/index.html#nicosia-bnp-and-a-fantastic-coffee",
    "title": "Cyprus",
    "section": "Nicosia, BNP and a fantastic coffee",
    "text": "Nicosia, BNP and a fantastic coffee\nI got into Nicosia quite late the day before the conference began; I hadn’t appreciated how far away Cyprus is, but the shuttle from Larnaca airport into Nicosia was cheap (I think 8 euro?) and pretty easy. I eventually got into my flat for the week which was down by Konstantinou & Elenis cemetary. I wanted to be near enough to both a park and the University where the conference was being hosted, and this fit the bill, as well as giving me some space where I could unwind, work and cook for myself.\nI displayed immense wisdom on the first morning. I didn’t go out for a run and instead made an approximation of shakshouka. The day was already set-up to be intensely hot. Everyday in Nicosia would hover around the \\(30^o\\)C mark for a few hours around midday; thankfully the theatre the conference talks were in was plenty cool.\nI think there was about 70 people in attendance in total, and it took awhile for everyone to trickle in. I was pretty excited for the tutorials, David would be opening and I had enjoyed previous talks of his (including one which I technically hosted), and his tutorial “Some musings on Bayesian non-parametrics” sounded open enough to accessible and interesting. I wasn’t familiar with Yanxun Xu, but her work was very applied so I expected to enjoy it. I was not sure how much of Aad’s tutorial I’d follow with a lack of technical background, but I was looking forward to the challenge as this is the man who wrote the book on BNP.\nMost of the talks were over my head throughout the conference, so I’m not going to even attempt discuss all of the talks as I could not do them justice. Instead I’ll mention a few I particularly resonated with:\n\nLasse Vuursteen, “A Bayesian approach to multiview learning”.\nJudith Rousseau, “Bayesian nonparametric estimation of a density living near an unknown manifold”.\nChris Holmes, “Why don’t Bayesians bootstrap?” (I think this is the appropriate paper).\nThibault Randrianarisoa, “Smoothing and adaptation of shifted Pólya Tree ensembles”.\nDeborah Sulem, “Bayesian nonparametric estimation of nonlinear Hawkes processes”.\nJordan Bryan, “The multirank likelihood and cyclically monotone Monte Carlo: a semiparametric approach to CCA”.\nMinh-Lien Jeanne Nguyen, “Bernstein-von Mises for semiparametric mixture”.\nMariia Vladimirova, “Bayesian Neural Networks at Finite and Infinite Widths”.\nDennis Nieman, “Contraction rates and uncertainty quantification for variational Bayes”.\n\nI unfortunately had to miss a set of talks to host an online talk from Trevor Campbell about his and his group’s work on Bayesian coresets. This loss was mitigated by how great a talk Trevor gave. The work itself is very interesting (hence why I had been attempting to wrangle an invite to Trevor to speak for awhile), but Trevor has also clearly thought a lot about how to convey information and how to present his work, so I really enjoyed it.\nMy own talk went down well, with both David and Sara Wade being particularly positive. It was really nice to have the opportunity to discuss the work a little with them both."
  }
]